# Genesis VLA Training System

**Complete pipeline for self-supervised embodied AI learning**

Bridge reality â†” Genesis simulation for Vision-Language-Action (VLA) training where the robot:
- Scans the real world with 3DOF orbital gimbal
- Converts to Genesis simulation with real geometry
- Trains VLA policies in ultra-fast physics simulation
- Transfers learned policies back to physical robot
- Continuously improves from embodied experience

---

## Table of Contents

- [Overview](#overview)
- [Why Genesis?](#why-genesis)
- [System Architecture](#system-architecture)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Complete Pipeline](#complete-pipeline)
- [VLA Training](#vla-training)
- [Sim2Real Transfer](#sim2real-transfer)
- [Advanced Topics](#advanced-topics)

---

## Overview

This system enables **foundation model training at the edge** - your robot becomes its own:

1. **Data Collector**: Orbital 3D scanning from all angles
2. **Scene Builder**: Converts real geometry to Genesis simulation
3. **Policy Trainer**: Learns VLA mappings in ultra-fast simulation
4. **Validator**: Executes and validates in reality
5. **Continuous Learner**: Improves from outcomes

### The Complete Loop

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   EMBODIED AI LEARNING LOOP                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    REALITY                GENESIS SIM            LEARNING

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  Orbital  â”‚           â”‚ Physics  â”‚          â”‚   VLA    â”‚
 â”‚   Scan   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Scene   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚  Policy  â”‚
 â”‚  (3DOF)  â”‚  Convert  â”‚  Build   â”‚  Train   â”‚ Training â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â†‘                                             â”‚
      â”‚                                             â”‚
      â”‚                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
      â”‚                 â”‚  Sim2Realâ”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚ Transfer â”‚â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            Execute     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  Deploy
            & Learn
```

### Key Innovation

**Complete Spatial Understanding** through orbital scanning:
- Not just "see object from front" â†’ "understand from ALL angles"
- Grounds language in complete 3D geometry
- Learns manipulation from full spatial context
- Trains with REAL geometry, not synthetic models

---

## Why Genesis?

[Genesis](https://genesis-world.readthedocs.io/) is a revolutionary physics simulation engine perfect for embodied AI:

### Performance
- âš¡ **430,000x faster** than real-time (world record)
- ðŸš€ **GPU-accelerated** parallel simulation
- ðŸŽ¯ **Differentiable** physics for end-to-end learning
- ðŸ’» **Runs on Jetson** Orin Nano (edge training!)

### Capabilities
- ðŸ¤– **Robotics**: Rigid bodies, articulated systems, contacts
- ðŸ’§ **Physics**: Soft bodies, fluids, particles, cloth
- ðŸ‘ï¸ **Rendering**: Photorealistic ray tracing
- ðŸ§  **AI-Native**: Python + PyTorch integration

### Perfect for VLA Training
- **Multi-view rendering** for vision encoding
- **Fast iteration** for policy learning
- **Real geometry** from orbital scans
- **Sim2Real** with accurate physics

---

## System Architecture

### Components

```python
whoami/
â”œâ”€â”€ gimbal_3dof.py              # 3DOF orbital scanning kinematics
â”œâ”€â”€ gimbal_3dof_controller.py   # Hardware control
â”œâ”€â”€ scanner_3d.py               # Point cloud capture
â”œâ”€â”€ genesis_bridge.py           # Reality â†” Genesis bridge (NEW!)
â”‚   â”œâ”€â”€ OrbitalToGenesis       # Scan â†’ Genesis converter
â”‚   â”œâ”€â”€ GenesisSceneBuilder    # Build simulation scenes
â”‚   â”œâ”€â”€ VLATrainingPipeline    # Train embodied policies
â”‚   â””â”€â”€ Sim2RealBridge         # Transfer to reality
â””â”€â”€ examples/
    â””â”€â”€ genesis_vla_training.py # Complete workflow examples
```

### Data Flow

1. **Perception** (Reality):
   ```
   3DOF Gimbal â†’ Orbital Scan â†’ Point Clouds + Images
   ```

2. **Digitization** (Reality â†’ Genesis):
   ```
   Point Clouds â†’ Mesh â†’ Physics Properties â†’ Genesis Scene
   ```

3. **Training** (Genesis):
   ```
   Multi-view Observations + Language â†’ VLA Policy â†’ Actions
   Simulate 1000s of episodes in minutes
   ```

4. **Transfer** (Genesis â†’ Reality):
   ```
   Learned Policy â†’ Real Robot â†’ Execute â†’ Observe Outcome
   ```

5. **Learning** (Continuous):
   ```
   Outcome â†’ Update World Model â†’ Improve Policy â†’ Repeat
   ```

---

## Installation

### Prerequisites

- Jetson Orin Nano with JetPack 5.1.2+
- CUDA 11.8+ (included with JetPack)
- Python 3.8-3.10
- WhoAmI system installed (see INSTALLATION.md)

### Install Genesis

```bash
# Activate virtual environment
source ~/whoami_env/bin/activate

# Install Genesis
pip install genesis-world

# Verify installation
python -c "import genesis as gs; print(f'Genesis {gs.__version__}')"
```

### Install Additional Dependencies

```bash
# For VLA training (optional, for full pipeline)
pip install torch torchvision  # If not already installed
pip install transformers  # For language encoding
pip install diffusers  # For diffusion policies
```

### Hardware Requirements

- **Minimum**: Jetson Orin Nano 8GB
- **Recommended**: Jetson AGX Orin for faster training
- **Storage**: 10GB+ for Genesis + models
- **GPU**: CUDA-capable (included in Jetson)

---

## Quick Start

### Test Genesis Installation

```bash
cd ~/whoami
python -c "
import genesis as gs
gs.init(backend=gs.gpu)
print('Genesis ready!')
"
```

### Run Basic Example

```bash
python examples/genesis_vla_training.py basic
```

This will:
1. Scan an object with orbital gimbal
2. Convert to Genesis simulation
3. Train a VLA policy
4. Execute in simulation

---

## Complete Pipeline

### Phase 1: Scan Real World

```python
from whoami.gimbal_3dof_controller import Gimbal3DOFController
from whoami.scanner_3d import Scanner3D
from whoami.genesis_bridge import OrbitalToGenesis

# Initialize hardware
gimbal = Gimbal3DOFController()
scanner = Scanner3D()
converter = OrbitalToGenesis()

gimbal.connect()
scanner.initialize()

# Scan object from all angles
object_center = np.array([0.30, 0.0, 0.0])

def capture_views(idx, angles):
    cloud = scanner.capture_point_cloud()
    image = scanner.capture_rgb_frame()
    return cloud, image

gimbal.scan_horizontal_orbit(
    center=object_center,
    radius=0.20,
    num_points=36,  # 36 views around object
    scan_callback=capture_views
)

# Generate mesh
mesh = scanner.generate_mesh(merged_clouds)

# Convert to Genesis
scanned_obj = converter.create_scanned_object(
    name="coffee_mug",
    mesh=mesh,
    material_hint="ceramic"
)
```

### Phase 2: Build Genesis Scene

```python
from whoami.genesis_bridge import GenesisSceneBuilder

# Initialize Genesis
builder = GenesisSceneBuilder()

# Build scene with scanned objects
scene = builder.create_scene(
    scene_id="tabletop",
    objects=[scanned_obj],
    ground_plane=True
)

# Scene is ready for simulation!
```

### Phase 3: Train VLA Policy

```python
from whoami.genesis_bridge import VLATrainingPipeline

# Initialize training pipeline
vla = VLATrainingPipeline(builder)

# Generate training data (runs in Genesis)
experiences = vla.generate_training_data(
    scanned_objects=[scanned_obj],
    language_instructions=["pick up the mug by the handle"],
    num_episodes=1000  # Ultra-fast in Genesis!
)

# Train policy
vla.train(experiences, num_epochs=100)

# Save
vla.save_model("models/mug_pickup_policy.pt")
```

### Phase 4: Execute in Reality

```python
# Load trained policy
vla.load_model("models/mug_pickup_policy.pt")

# Execute on real robot
# (Integration with robot control system)
result = execute_policy_on_robot(
    policy=vla,
    instruction="pick up the mug by the handle",
    object_center=object_center
)

print(f"Success: {result['success']}")
```

### Phase 5: Continuous Learning

```python
# Update world model from outcome
update_world_model(result)

# Rescan if needed (geometry changed)
if object_moved:
    new_scan = scan_and_digitize_object(...)
    update_genesis_scene(new_scan)

# Fine-tune policy
fine_tune_from_experience(result)

# Repeat!
```

---

## VLA Training

### Vision-Language-Action Architecture

```
Multi-View Images (from orbital scan)
    â†“
Vision Encoder (Multi-view Transformer)
    â†“ (512-dim embedding)
    â”œâ”€â†’ Spatial Understanding
    â”‚
Language Instruction ("pick up the mug")
    â†“
Language Encoder (BERT/T5)
    â†“ (768-dim embedding)
    â”œâ”€â†’ Semantic Grounding
    â”‚
    â†“ (Fusion)
Action Policy (Diffusion/Flow Matching)
    â†“
Actions (6-DOF poses + gripper)
```

### What the Robot Learns

1. **Visual Understanding**:
   - Object geometry from all angles
   - Graspable surfaces and affordances
   - Spatial relationships
   - Material properties (from physics)

2. **Language Grounding**:
   - "the handle" â†’ specific mesh region
   - "pick up" â†’ grasp + lift action sequence
   - "carefully" â†’ slow, gentle movements
   - Grounds language in complete 3D understanding

3. **Action Planning**:
   - Approach trajectories considering full geometry
   - Grasp points from multi-view analysis
   - Manipulation sequences
   - Error recovery behaviors

4. **Physics Understanding**:
   - Object dynamics (learned in Genesis)
   - Contact forces
   - Stability constraints
   - Realistic motion planning

### Training Data Generation

Genesis enables massive parallel training:

```python
# Generate 10,000 episodes in minutes!
for episode in parallel_environments(10000):
    # Randomize object pose
    reset_scene_random()

    # Run episode
    obs = get_multi_view_observation()
    action = policy(obs, instruction)
    next_obs, reward = simulate_action(action)

    # Store experience
    replay_buffer.add(obs, action, reward, next_obs)

# Train on diverse experiences
```

---

## Sim2Real Transfer

### Why It Works

**Real Geometry**: Training with actual scanned objects (not synthetic models) dramatically improves transfer

**Key Strategies**:

1. **Domain Randomization** (in Genesis):
   ```python
   # Randomize physics
   mass = sample(0.8 * real_mass, 1.2 * real_mass)
   friction = sample(0.4, 0.7)

   # Randomize appearance
   lighting = random_lighting()
   textures = random_textures()

   # Randomize sensor noise
   depth_noise = gaussian(mean=0, std=0.01)
   ```

2. **Multi-View Consistency**:
   - Train on ALL angles (from orbital scan)
   - Policy learns view-invariant features
   - Works regardless of camera position

3. **Physics Validation**:
   - Compare sim and real dynamics
   - Measure discrepancy
   - Adjust simulation parameters

4. **Progressive Transfer**:
   ```
   Genesis (1000 episodes) â†’ Safety-filtered
   â†“
   Simulation Validation â†’ Test policy in sim
   â†“
   Limited Real Trials (10 episodes) â†’ Safe execution
   â†“
   Full Deployment â†’ Continuous monitoring
   ```

### Validation

```python
from whoami.genesis_bridge import Sim2RealBridge

bridge = Sim2RealBridge()

# Validate transfer
metrics = bridge.validate_sim2real(
    policy=trained_policy,
    real_observations=[...],
    sim_scene_id="tabletop"
)

print(f"Success rate: {metrics['success_rate']}")
print(f"Sim/Real discrepancy: {metrics['sim_real_discrepancy']}")
```

---

## Advanced Topics

### Custom VLA Architectures

Implement your own VLA model:

```python
class CustomVLAPolicy:
    def __init__(self):
        self.vision_encoder = MultiViewTransformer()
        self.language_encoder = BERTEncoder()
        self.action_decoder = DiffusionPolicy()

    def forward(self, images, instruction):
        # Encode multi-view images
        visual_features = self.vision_encoder(images)

        # Encode language
        language_features = self.language_encoder(instruction)

        # Fuse
        combined = fuse_features(visual_features, language_features)

        # Decode action
        action = self.action_decoder(combined)

        return action
```

### Multi-Task Learning

Train on multiple tasks simultaneously:

```python
tasks = [
    "pick up the mug",
    "stack the blocks",
    "pour water",
    "open the drawer"
]

# Generate data for all tasks
experiences = {}
for task in tasks:
    experiences[task] = generate_training_data(
        task_description=task,
        num_episodes=1000
    )

# Multi-task training
train_multitask(experiences)
```

### Curriculum Learning

Start simple, progress to complex:

```python
curriculum = [
    {"stage": 1, "complexity": "single_object", "episodes": 1000},
    {"stage": 2, "complexity": "two_objects", "episodes": 2000},
    {"stage": 3, "complexity": "clutter", "episodes": 5000},
    {"stage": 4, "complexity": "dynamic_obstacles", "episodes": 10000}
]

for stage in curriculum:
    logger.info(f"Training stage {stage['stage']}")
    train_on_complexity_level(stage)
```

### Real-Time World Model

Maintain synchronized sim/real world state:

```python
class WorldModel:
    def __init__(self):
        self.genesis_scene = None
        self.real_state = None

    def update_from_scan(self, scanned_object):
        """Update Genesis scene with new scan"""
        # Update object geometry
        self.genesis_scene.update_object(scanned_object)

        # Re-sync physics
        self.genesis_scene.rebuild()

    def predict_outcome(self, action):
        """Simulate action in Genesis"""
        predicted_state = self.genesis_scene.simulate(action)
        return predicted_state

    def validate_prediction(self, real_outcome):
        """Compare prediction with reality"""
        error = compute_error(predicted_state, real_outcome)
        if error > threshold:
            self.recalibrate_physics()
```

### Autonomous Exploration

Robot decides what to scan and learn:

```python
class AutonomousExplorer:
    def select_next_object(self):
        """Choose object to explore based on curiosity"""
        objects = detect_objects_in_scene()

        # Compute information gain
        scores = []
        for obj in objects:
            uncertainty = self.estimate_uncertainty(obj)
            novelty = self.compute_novelty(obj)
            scores.append(uncertainty * novelty)

        # Select most interesting
        best_object = objects[np.argmax(scores)]
        return best_object

    def explore_and_learn(self):
        while True:
            # Select object
            obj = self.select_next_object()

            # Scan it
            scan_data = self.scan_object(obj)

            # Build Genesis scene
            scene = self.build_scene(scan_data)

            # Train policy
            self.train_interaction_policy(scene)

            # Execute and validate
            result = self.execute_in_reality()

            # Learn from outcome
            self.update_world_model(result)
```

---

## Troubleshooting

### Genesis Not Installing

```bash
# Ensure CUDA is available
nvidia-smi

# Check Python version
python --version  # Should be 3.8-3.10

# Try with specific CUDA version
pip install genesis-world --index-url https://pypi.org/simple/

# Or build from source
git clone https://github.com/Genesis-Embodied-AI/Genesis.git
cd Genesis
pip install -e .
```

### Out of GPU Memory

```bash
# Reduce batch size
config["training"]["batch_size"] = 8  # Smaller batches

# Reduce number of parallel environments
num_envs = 10  # Instead of 100

# Use gradient accumulation
accumulation_steps = 4
```

### Sim2Real Gap Too Large

```python
# Increase domain randomization
randomization_config = {
    "physics_variation": 0.3,  # Â±30% physics parameters
    "appearance_variation": 0.5,  # More visual variation
    "sensor_noise": 0.02  # Higher sensor noise
}

# Collect more real-world data
real_data = collect_real_demonstrations(num_episodes=100)

# Fine-tune on real data
fine_tune_on_real_data(policy, real_data)
```

### Slow Training

```python
# Use multiple GPUs (if available)
genesis.init(backend=gs.gpu, num_gpus=2)

# Increase parallel environments
num_parallel = 1000  # More environments

# Use faster policy architecture
policy = "mlp"  # Instead of "transformer"

# Reduce simulation fidelity (if acceptable)
timestep = 1.0/30.0  # 30 Hz instead of 60 Hz
```

---

## Performance Benchmarks

### Jetson Orin Nano (8GB)

| Task | Performance |
|------|-------------|
| Orbital Scan (36 views) | 3-5 minutes |
| Mesh Generation | 10-30 seconds |
| Genesis Scene Build | 2-5 seconds |
| Training (1000 episodes) | 5-15 minutes |
| Policy Inference | <100ms |

### Training Speed Comparison

| Method | Episodes/Hour | Comments |
|--------|---------------|----------|
| Real Robot | 10 | Slow, expensive |
| Traditional Sim | 1,000 | PyBullet/MuJoCo |
| **Genesis** | **100,000+** | 430,000x real-time! |

---

## Future Directions

### Foundation Models

Train on diverse objects and tasks to create general-purpose manipulation models:

```python
# Train on 1000s of objects
for obj_category in ["kitchen", "tools", "toys", "office"]:
    scan_and_train_on_category(obj_category)

# Result: General manipulation policy
policy = train_foundation_model(all_experiences)
```

### Multi-Modal Learning

Combine vision, language, touch, audio:

```python
observation = {
    "vision": orbital_images,
    "language": task_description,
    "touch": tactile_feedback,
    "audio": contact_sounds
}

action = multimodal_policy(observation)
```

### Collaborative Learning

Multiple robots share experiences:

```python
# Robot A scans object 1
scan_a = robot_a.scan_object(obj_1)

# Robot B uses Robot A's scan
robot_b.load_scan(scan_a)
robot_b.train_policy(scan_a)

# Shared learning!
```

---

## Quick Reference

### Essential Commands

```bash
# Install Genesis
pip install genesis-world

# Test installation
python -c "import genesis as gs; gs.init(backend=gs.gpu)"

# Run basic workflow
python examples/genesis_vla_training.py basic

# Run multi-object training
python examples/genesis_vla_training.py multi

# Run continuous learning
python examples/genesis_vla_training.py continuous
```

### Key Files

- **Bridge**: `whoami/genesis_bridge.py`
- **Examples**: `examples/genesis_vla_training.py`
- **Config**: `config/genesis_config.json` (to be created)
- **Models**: `models/vla_*` (saved policies)

---

## Resources

- **Genesis Docs**: https://genesis-world.readthedocs.io/
- **Genesis Paper**: https://arxiv.org/abs/2411.00735
- **Genesis GitHub**: https://github.com/Genesis-Embodied-AI/Genesis
- **WhoAmI Docs**: `docs/` directory

---

**This system represents the future of embodied AI**: robots that learn from complete spatial understanding through self-supervised exploration in ultra-fast simulation! ðŸ¤–ðŸŒâœ¨

---

**Version**: 1.0
**Last Updated**: 2025-11-07
**For**: Jetson Orin Nano + Genesis + 3DOF Gimbal + OAK-D S3
